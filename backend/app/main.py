#!/usr/bin/env python3
"""
Backend FastAPI ‚Äî Version Unifi√©e Atelier IA + Contexte Conversationnel
Supporte: workflows, chat agent avec m√©moire, upload fichiers, multi-agent, Ollama.
VERSION STABLE CORRIG√âE AVEC OLLAMA ‚Äî 2024
"""

from fastapi import FastAPI, HTTPException, Request, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import time, uuid, os, traceback, logging, shutil, asyncio, json
try:
    import httpx
except ImportError:
    httpx = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("atelier-backend")

app = FastAPI(title="Backend Atelier IA Unifi√©", version="1.4.0")

# ---- CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===================
# == CONFIGURATION ==
# ===================

OLLAMA_URL = "http://localhost:11434"
DEFAULT_MODEL = "llama3-chatqa:latest"
UPLOAD_DIR = "./uploaded_files"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# ===================
# == OLLAMA SERVICE ==
# ===================

class OllamaService:
    def __init__(self, base_url: str = OLLAMA_URL):
        self.base_url = base_url
        self.available_models = []
        
    async def check_connection(self) -> bool:
        """V√©rifie si Ollama est accessible"""
        if not httpx:
            logger.warning("httpx non install√© - Ollama d√©sactiv√©")
            return False
            
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags", timeout=5.0)
                if response.status_code == 200:
                    data = response.json()
                    self.available_models = [model["name"] for model in data.get("models", [])]
                    return True
                return False
        except Exception as e:
            logger.error(f"Ollama connection failed: {e}")
            return False
    
    async def generate(self, model: str, prompt: str, stream: bool = False) -> str:
        """G√©n√®re une r√©ponse avec Ollama"""
        if not httpx:
            return f"[Mock] R√©ponse pour le mod√®le {model}: {prompt[:50]}..."
            
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                payload = {
                    "model": model,
                    "prompt": prompt,
                    "stream": stream,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "num_ctx": 4096
                    }
                }
                
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json=payload
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data.get("response", "Pas de r√©ponse g√©n√©r√©e")
                else:
                    logger.error(f"Ollama error: {response.status_code} - {response.text}")
                    return f"Erreur Ollama: {response.status_code}"
                    
        except asyncio.TimeoutError:
            return "Timeout: La g√©n√©ration a pris trop de temps"
        except Exception as e:
            logger.error(f"Ollama generation error: {e}")
            return f"Erreur de g√©n√©ration: {str(e)}"
    
    async def get_models(self) -> List[str]:
        """R√©cup√®re la liste des mod√®les disponibles"""
        if not httpx:
            return ["llama3.1", "codellama", "mistral"]  # Mock models
            
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags", timeout=10.0)
                if response.status_code == 200:
                    data = response.json()
                    return [model["name"] for model in data.get("models", [])]
                return []
        except Exception as e:
            logger.error(f"Failed to get models: {e}")
            return []

# Instance globale Ollama
ollama_service = OllamaService()

# ===================
# == AGENT SERVICES ==
# ===================

def get_available_agents():
    """Retourne la liste des agents disponibles"""
    return ["assistant", "code-assistant", "debugger", "reviewer", "optimizer", "documentation"]

async def query_agent(agent_role: str, message: str, context: Dict[str, Any] = None) -> str:
    """Service principal pour interroger les agents IA via Ollama"""
    try:
        # V√©rifier la connexion Ollama
        if not await ollama_service.check_connection():
            logger.warning("Ollama non disponible, utilisation du mode mock")
            return await query_agent_mock(agent_role, message, context)
        
        # S√©lectionner le mod√®le selon l'agent
        model = get_model_for_agent(agent_role)
        
        # Construire le prompt sp√©cialis√© selon l'agent
        specialized_prompt = build_agent_prompt(agent_role, message, context)
        
        # G√©n√©rer la r√©ponse avec Ollama
        response = await ollama_service.generate(model, specialized_prompt)
        
        return response
        
    except Exception as e:
        logger.error(f"Erreur query_agent: {e}")
        # Fallback vers le mock en cas d'erreur
        return await query_agent_mock(agent_role, message, context)

def get_model_for_agent(agent_role: str) -> str:
    """S√©lectionne le mod√®le optimal selon l'agent"""
    model_mapping = {
        "code-assistant": DEFAULT_MODEL,
        "debugger": DEFAULT_MODEL,
        "reviewer": DEFAULT_MODEL,
        "optimizer": DEFAULT_MODEL,
        "documentation": DEFAULT_MODEL,
        "assistant": DEFAULT_MODEL
    }
    return model_mapping.get(agent_role, DEFAULT_MODEL)

def build_agent_prompt(agent_role: str, message: str, context: Dict[str, Any] = None) -> str:
    """Construit un prompt sp√©cialis√© selon le r√¥le de l'agent"""
    
    # Prompts syst√®me selon l'agent
    system_prompts = {
        "assistant": "Tu es un assistant IA utile et bienveillant. R√©ponds de mani√®re claire et pr√©cise.",
        
        "code-assistant": """Tu es un expert en d√©veloppement logiciel. Tu aides √†:
- G√©n√©rer du code propre et fonctionnel
- R√©soudre des probl√®mes de programmation
- Expliquer des concepts techniques
- Optimiser les performances
R√©ponds toujours avec du code comment√© et des explications claires.""",

        "debugger": """Tu es un expert en debugging. Tu aides √†:
- Identifier les bugs dans le code
- Analyser les erreurs et exceptions
- Proposer des solutions de correction
- Optimiser le code pour √©viter les erreurs
Sois m√©thodique et pr√©cis dans tes analyses.""",

        "reviewer": """Tu es un expert en review de code. Tu √©values:
- La qualit√© du code et les bonnes pratiques
- La s√©curit√© et les vuln√©rabilit√©s
- Les performances et l'optimisation
- La lisibilit√© et la maintenabilit√©
Donne des commentaires constructifs et des suggestions d'am√©lioration.""",

        "optimizer": """Tu es un expert en optimisation. Tu te concentres sur:
- L'am√©lioration des performances
- La r√©duction de la complexit√©
- L'optimisation des ressources
- Les algorithmes plus efficaces
Propose des solutions concr√®tes et mesurables.""",

        "documentation": """Tu es un expert en documentation technique. Tu aides √†:
- Cr√©er une documentation claire et compl√®te
- R√©diger des commentaires de code
- Expliquer des architectures complexes
- Cr√©er des guides d'utilisation
√âcris de mani√®re structur√©e et accessible."""
    }
    
    system_prompt = system_prompts.get(agent_role, system_prompts["assistant"])
    
    # Construire le prompt final
    prompt_parts = [system_prompt, "\n"]
    
    # Ajouter le contexte si disponible
    if context:
        if context.get("recentMessages"):
            prompt_parts.append("HISTORIQUE DE LA CONVERSATION:")
            for msg in context["recentMessages"][-3:]:  # 3 derniers messages
                if isinstance(msg, dict):
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    prompt_parts.append(f"{role.upper()}: {content}")
            prompt_parts.append("")
        
        if context.get("codeContext"):
            prompt_parts.append(f"FICHIERS DE CODE: {', '.join(context['codeContext'])}")
        
        if context.get("projectId"):
            prompt_parts.append(f"PROJET: {context['projectId']}")
        
        prompt_parts.append("")
    
    # Ajouter la demande actuelle
    prompt_parts.append(f"DEMANDE:\n{message}")
    
    return "\n".join(prompt_parts)

async def query_agent_mock(agent_role: str, message: str, context: Dict[str, Any] = None) -> str:
    """Service mock pour les agents (fallback)"""
    await asyncio.sleep(0.1)  # Simulation d√©lai
    
    context_info = ""
    if context:
        msg_count = len(context.get('recentMessages', []))
        project = context.get('projectId', 'Aucun')
        context_info = f" (Contexte: {msg_count} messages, Projet: {project})"
    
    responses = {
        "code-assistant": f"[Code Assistant] Je vais vous aider avec: {message[:60]}...{context_info}\n\nVoici une r√©ponse de d√©veloppement structur√©e pour votre demande.",
        "debugger": f"[Debugger] Analyse du probl√®me: {message[:60]}...{context_info}\n\nJe vais identifier les issues potentielles et proposer des solutions.",
        "reviewer": f"[Reviewer] Review de code: {message[:60]}...{context_info}\n\nVoici mes recommandations pour am√©liorer la qualit√© du code.",
        "optimizer": f"[Optimizer] Optimisation: {message[:60]}...{context_info}\n\nJe propose ces am√©liorations de performance.",
        "documentation": f"[Documentation] Documentation: {message[:60]}...{context_info}\n\nVoici une documentation structur√©e pour votre projet.",
        "assistant": f"[Assistant] Je comprends votre demande: {message[:60]}...{context_info}\n\nVoici ma r√©ponse d√©taill√©e."
    }
    
    return responses.get(agent_role, responses["assistant"])

# ===================
# == WORKFLOW ENGINE ==
# ===================

class WorkflowOrchestrator:
    def __init__(self):
        self.running_workflows = {}
    
    def get_available_workflows(self):
        return [
            "code_generation", 
            "project_setup", 
            "debugging", 
            "optimization",
            "documentation_generation",
            "code_review",
            "testing"
        ]
    
    async def start_workflow(self, workflow_type: str, description: str):
        workflow_id = f"wf_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        self.running_workflows[workflow_id] = {
            "type": workflow_type,
            "status": "running",
            "description": description,
            "created_at": datetime.now().isoformat(),
            "steps": []
        }
        
        # Simuler l'ex√©cution du workflow
        asyncio.create_task(self._execute_workflow(workflow_id, workflow_type, description))
        
        return workflow_id
    
    async def _execute_workflow(self, workflow_id: str, workflow_type: str, description: str):
        """Ex√©cute un workflow de mani√®re asynchrone"""
        try:
            # Simuler des √©tapes de workflow
            steps = self._get_workflow_steps(workflow_type)
            
            for i, step in enumerate(steps):
                await asyncio.sleep(2)  # Simulation d√©lai
                self.running_workflows[workflow_id]["steps"].append({
                    "step": i + 1,
                    "name": step,
                    "status": "completed",
                    "timestamp": datetime.now().isoformat()
                })
            
            self.running_workflows[workflow_id]["status"] = "completed"
            
        except Exception as e:
            logger.error(f"Workflow {workflow_id} failed: {e}")
            self.running_workflows[workflow_id]["status"] = "failed"
            self.running_workflows[workflow_id]["error"] = str(e)
    
    def _get_workflow_steps(self, workflow_type: str) -> List[str]:
        """Retourne les √©tapes d'un type de workflow"""
        steps_mapping = {
            "code_generation": ["Analyse des requirements", "G√©n√©ration du code", "Tests", "Documentation"],
            "project_setup": ["Initialisation", "Structure de dossiers", "Configuration", "D√©pendances"],
            "debugging": ["Analyse du probl√®me", "Identification des causes", "Correction", "Validation"],
            "optimization": ["Analyse des performances", "Identification des goulots", "Optimisation", "Benchmarking"],
            "documentation_generation": ["Analyse du code", "G√©n√©ration des docs", "R√©vision", "Publication"],
            "code_review": ["Analyse statique", "Review s√©curit√©", "Review performance", "Rapport final"],
            "testing": ["Analyse du code", "G√©n√©ration des tests", "Ex√©cution", "Rapport de couverture"]
        }
        return steps_mapping.get(workflow_type, ["√âtape 1", "√âtape 2", "Finalisation"])
    
    async def get_workflow_status(self, workflow_id: str):
        if workflow_id in self.running_workflows:
            return self.running_workflows[workflow_id]
        raise Exception("Workflow not found")
    
    async def stop_workflow(self, workflow_id: str):
        if workflow_id in self.running_workflows:
            self.running_workflows[workflow_id]["status"] = "stopped"
            return True
        return False

workflow_orchestrator = WorkflowOrchestrator()

# ===================
# == MODELS ========
# ===================

class WorkflowRequest(BaseModel):
    workflow_type: str
    description: str

class ConversationContext(BaseModel):
    summary: Optional[str] = None
    recentMessages: Optional[List[Dict[str, Any]]] = []
    codeContext: Optional[List[str]] = []
    objectives: Optional[List[str]] = []
    currentFiles: Optional[List[str]] = []
    conversationId: Optional[str] = None
    projectId: Optional[str] = None
    messageCount: Optional[int] = 0
    lastInteraction: Optional[str] = None

class AgentChatRequest(BaseModel):
    message: str
    agent: Optional[str] = "assistant"
    context: Optional[Union[Dict[str, Any], ConversationContext]] = None
    project_id: Optional[str] = None
    conversation_id: Optional[str] = None

class ChatMessage(BaseModel):
    message: str
    agent: Optional[str] = "assistant"
    context: Optional[Union[Dict[str, Any], ConversationContext]] = None
    project_id: Optional[str] = None
    conversation_id: Optional[str] = None
    execute_code: Optional[bool] = False
    language: Optional[str] = None

class CodeExecutionRequest(BaseModel):
    code: str
    language: str
    agent: Optional[str] = "code-assistant"
    context: Optional[Union[Dict[str, Any], ConversationContext]] = None

class CodeAnalysisRequest(BaseModel):
    code: str
    language: str
    analysis_type: Optional[str] = "review"

class CodeGenerationRequest(BaseModel):
    description: str
    language: str
    context: Optional[Union[Dict[str, Any], ConversationContext]] = None

class ModelSwitchRequest(BaseModel):
    model: str

# ===================
# == UTILS CONTEXTE ==
# ===================

def safe_get_context_dict(context: Union[Dict, ConversationContext, None]) -> Dict[str, Any]:
    """Convertit le contexte en dictionnaire de mani√®re s√©curis√©e"""
    if context is None:
        return {}
    
    try:
        if isinstance(context, dict):
            return context
        elif hasattr(context, 'dict'):
            return context.dict()
        else:
            return {}
    except Exception as e:
        logger.error(f"Erreur conversion contexte: {e}")
        return {}

def build_contextual_prompt(message: str, context: Union[Dict, ConversationContext, None] = None) -> str:
    """Construit un prompt enrichi avec le contexte conversationnel"""
    if not context:
        return message
    
    try:
        context_dict = safe_get_context_dict(context)
        prompt_parts = []
        
        # Ajouter le r√©sum√© de la conversation
        if context_dict.get("summary"):
            prompt_parts.append(f"CONTEXTE DE LA CONVERSATION:\n{context_dict['summary']}\n")
        
        # Ajouter les messages r√©cents
        recent_messages = context_dict.get("recentMessages", [])
        if recent_messages and isinstance(recent_messages, list) and len(recent_messages) > 0:
            prompt_parts.append("HISTORIQUE R√âCENT:")
            for msg in recent_messages[-5:]:  # 5 derniers messages
                if isinstance(msg, dict):
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    if content:
                        prompt_parts.append(f"{role.upper()}: {content}")
            prompt_parts.append("")
        
        # Ajouter le contexte de code
        code_context = context_dict.get("codeContext", [])
        if code_context and isinstance(code_context, list) and len(code_context) > 0:
            prompt_parts.append(f"FICHIERS DE CODE ACTUELS: {', '.join(code_context)}")
        
        # Ajouter les objectifs
        objectives = context_dict.get("objectives", [])
        if objectives and isinstance(objectives, list) and len(objectives) > 0:
            prompt_parts.append(f"OBJECTIFS: {', '.join(objectives)}")
        
        # Ajouter les m√©tadonn√©es
        if context_dict.get("projectId"):
            prompt_parts.append(f"PROJET: {context_dict['projectId']}")
        
        if context_dict.get("messageCount"):
            prompt_parts.append(f"MESSAGE #{context_dict['messageCount']} de cette conversation")
        
        # Ajouter le message actuel
        prompt_parts.append(f"\nDEMANDE ACTUELLE:\n{message}")
        
        return "\n".join(prompt_parts)
        
    except Exception as e:
        logger.error(f"Erreur build_contextual_prompt: {e}")
        return message

def update_context_with_response(context: Union[Dict, ConversationContext, None], user_message: str, ai_response: str) -> Dict[str, Any]:
    """Met √† jour le contexte avec la nouvelle interaction"""
    try:
        # Convertir le contexte en dict de mani√®re s√©curis√©e
        context_dict = safe_get_context_dict(context)
        
        # Initialiser recentMessages si n√©cessaire
        if "recentMessages" not in context_dict:
            context_dict["recentMessages"] = []
        
        # S'assurer que c'est une liste
        if not isinstance(context_dict["recentMessages"], list):
            context_dict["recentMessages"] = []
        
        # Ajouter les nouveaux messages
        new_messages = [
            {
                "role": "user",
                "content": user_message,
                "timestamp": datetime.now().isoformat()
            },
            {
                "role": "assistant", 
                "content": ai_response,
                "timestamp": datetime.now().isoformat()
            }
        ]
        
        context_dict["recentMessages"].extend(new_messages)
        
        # Garder seulement les 10 derniers messages
        context_dict["recentMessages"] = context_dict["recentMessages"][-10:]
        
        # Mettre √† jour les m√©tadonn√©es
        context_dict["lastInteraction"] = datetime.now().isoformat()
        context_dict["messageCount"] = context_dict.get("messageCount", 0) + 1
        
        return context_dict
        
    except Exception as e:
        logger.error(f"Erreur update_context_with_response: {e}")
        # Retourner un contexte minimal en cas d'erreur
        return {
            "recentMessages": [
                {"role": "user", "content": user_message, "timestamp": datetime.now().isoformat()},
                {"role": "assistant", "content": ai_response, "timestamp": datetime.now().isoformat()}
            ],
            "messageCount": 1,
            "lastInteraction": datetime.now().isoformat()
        }

# ===================
# == MIDDLEWARE ====
# ===================

@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    logger.info(f"‚û°Ô∏è {request.method} {request.url}")
    try:
        response = await call_next(request)
        duration = time.time() - start_time
        logger.info(f"‚úÖ {request.method} {request.url} [{response.status_code}] {duration:.2f}s")
        return response
    except Exception as e:
        duration = time.time() - start_time
        logger.error(f"‚ùå {request.method} {request.url} ERROR: {str(e)} {duration:.2f}s")
        return JSONResponse(status_code=500, content={"detail": str(e)})

# ===================
# == ROUTES API ====
# ===================

@app.get("/")
async def root():
    ollama_status = "‚úÖ Connect√©" if await ollama_service.check_connection() else "‚ùå D√©connect√©"
    
    return {
        "message": "Backend Atelier IA (unifi√© avec contexte conversationnel + Ollama)",
        "status": "ok",
        "version": "1.4.0",
        "datetime": datetime.now().isoformat(),
        "ollama_status": ollama_status,
        "available_models": ollama_service.available_models,
        "features": [
            "Chat avec m√©moire conversationnelle",
            "Int√©gration Ollama compl√®te",
            "Multi-agents sp√©cialis√©s",
            "Analyse et g√©n√©ration de code",
            "Workflows automatis√©s",
            "Upload de fichiers"
        ],
        "routes": [
            "/health", "/test", "/agents", "/agent", "/chat",
            "/agent/execute", "/agent/analyze", "/agent/generate",
            "/models/available", "/models/switch",
            "/workflows/available", "/workflows/start", "/workflows/{id}/status", 
            "/workflows", "/kb/upload"
        ],
        "agents_available": len(get_available_agents()),
        "workflows_running": len(workflow_orchestrator.running_workflows)
    }

@app.get("/health")
async def health():
    try:
        agents_count = len(get_available_agents())
        workflows_count = len(workflow_orchestrator.running_workflows)
        ollama_connected = await ollama_service.check_connection()
        
        return {
            "status": "healthy" if ollama_connected else "degraded",
            "timestamp": datetime.now().isoformat(),
            "services": {
                "agents": f"{agents_count} disponibles",
                "workflows": f"{workflows_count} actifs", 
                "storage": "disponible",
                "contexte_conversationnel": "activ√©",
                "ollama": "connect√©" if ollama_connected else "d√©connect√©",
                "models": f"{len(ollama_service.available_models)} mod√®les"
            },
            "version": "1.4.0"
        }
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return {
            "status": "degraded",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }

@app.get("/test")
async def test():
    ollama_test = await ollama_service.check_connection()
    return {
        "result": "API OK", 
        "timestamp": datetime.now().isoformat(),
        "test_status": "‚úÖ Toutes les fonctions op√©rationnelles",
        "contexte_test": "M√©moire conversationnelle active",
        "ollama_test": "‚úÖ Connect√©" if ollama_test else "‚ùå D√©connect√©"
    }

# ---- MODELS API ----

@app.get("/models/available")
async def get_available_models():
    """Liste des mod√®les Ollama disponibles"""
    try:
        models = await ollama_service.get_models()
        return {
            "success": True,
            "models": models,
            "default_model": DEFAULT_MODEL,
            "count": len(models),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Erreur liste mod√®les: {e}")
        return {
            "success": False,
            "models": [],
            "error": str(e)
        }

@app.post("/models/switch")
async def switch_model(request: ModelSwitchRequest):
    """Change le mod√®le par d√©faut"""
    try:
        available_models = await ollama_service.get_models()
        if request.model not in available_models:
            raise HTTPException(status_code=400, detail=f"Mod√®le '{request.model}' non disponible")
        
        global DEFAULT_MODEL
        DEFAULT_MODEL = request.model
        
        return {
            "success": True,
            "new_default_model": DEFAULT_MODEL,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Erreur changement mod√®le: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ---- AGENTS API ----

@app.get("/agents")
async def list_agents():
    """Liste tous les agents disponibles"""
    try:
        agents = get_available_agents()
        return {
            "success": True,
            "agents": agents,
            "count": len(agents),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Erreur liste agents: {e}")
        return {
            "success": False,
            "agents": [],
            "error": str(e)
        }

@app.get("/agents/{agent_id}/capabilities")
async def get_agent_capabilities(agent_id: str):
    """R√©cup√®re les capacit√©s d'un agent sp√©cifique"""
    available_agents = get_available_agents()
    if agent_id not in available_agents:
        raise HTTPException(status_code=404, detail=f"Agent '{agent_id}' non trouv√©")
    
    capabilities = {
        "assistant": ["conversation", "aide_generale", "explication", "conseil"],
        "code-assistant": ["generation_code", "review_code", "debug", "optimisation", "refactoring"],
        "debugger": ["analyse_erreurs", "debug", "tests", "correction_bugs"],
        "reviewer": ["review_code", "suggestions", "bonnes_pratiques", "securite"],
        "optimizer": ["optimisation", "performance", "refactoring", "algorithmes"],
        "documentation": ["redaction", "commentaires", "guides", "architecture"]
    }
    
    return {
        "agent_id": agent_id,
        "capabilities": capabilities.get(agent_id, ["conversation"]),
        "status": "available",
        "model": get_model_for_agent(agent_id),
        "timestamp": datetime.now().isoformat()
    }

# ---- CHAT API ----

@app.get("/chat")
async def chat_info():
    """Informations sur l'endpoint chat"""
    return {
        "endpoint": "/chat",
        "method": "POST",
        "description": "Endpoint pour communiquer avec les agents IA avec m√©moire conversationnelle",
        "usage": {
            "url": "http://localhost:8000/chat",
            "method": "POST",
            "content_type": "application/json",
            "body_example": {
                "message": "Continue le d√©veloppement du composant React",
                "agent": "code-assistant",
                "context": {
                    "summary": "D√©veloppement d'un composant React",
                    "recentMessages": [
                        {"role": "user", "content": "Cr√©e un composant Button"},
                        {"role": "assistant", "content": "Voici le composant..."}
                    ],
                    "codeContext": ["Button.tsx", "styles.css"],
                    "projectId": "mon-projet",
                    "conversationId": "conv-123"
                }
            }
        },
        "features": [
            "M√©moire conversationnelle",
            "Contexte de code",
            "Multi-agents sp√©cialis√©s",
            "Int√©gration Ollama",
            "Persistance des conversations"
        ],
        "available_agents": get_available_agents(),
        "timestamp": datetime.now().isoformat()
    }

@app.post("/chat")
async def chat(message: ChatMessage):
    """Endpoint principal pour le chat avec m√©moire conversationnelle"""
    try:
        # Validation
        if not message.message or not message.message.strip():
            raise HTTPException(status_code=400, detail="Le message ne peut pas √™tre vide")

        available_agents = get_available_agents()
        agent = message.agent or "assistant"
        if agent not in available_agents:
            raise HTTPException(status_code=400, detail=f"Agent '{agent}' non disponible. Agents: {available_agents}")

        start_time = time.time()
        
        # Construire le prompt avec contexte
        contextual_prompt = build_contextual_prompt(message.message, message.context)
        
        logger.info(f"ü§ñ Chat avec contexte: [{agent}] {message.message[:50]}...")
        if message.context:
            context_dict = safe_get_context_dict(message.context)
            if context_dict.get("conversationId"):
                logger.info(f"üìù Conversation: {context_dict['conversationId']}")

        # Appel √† l'agent avec le prompt contextualis√©
        context_dict = safe_get_context_dict(message.context)
        result = await query_agent(
            agent_role=agent,
            message=contextual_prompt,
            context=context_dict
        )

        duration = time.time() - start_time
        
        # Mettre √† jour le contexte avec la nouvelle interaction
        updated_context = update_context_with_response(
            message.context,
            message.message,
            result
        )

        logger.info(f"‚úÖ Agent [{agent}] responded in {duration:.2f}s")

        return {
            "success": True,
            "response": result,
            "agent": agent,
            "context": updated_context,  # Contexte mis √† jour pour le frontend
            "metadata": {
                "response_time": f"{duration:.2f}s",
                "message_count": updated_context.get("messageCount", 0),
                "has_context": bool(message.context),
                "project_id": message.project_id,
                "conversation_id": message.conversation_id,
                "model_used": get_model_for_agent(agent)
            },
            "timestamp": datetime.now().isoformat()
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Chat endpoint error: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Erreur chat: {str(e)}")

# ---- LEGACY SUPPORT ----

@app.post("/agent")
async def agent_chat_legacy(body: AgentChatRequest):
    """Endpoint legacy pour compatibilit√©"""
    try:
        start_time = time.time()
        logger.info(f"ü§ñ Agent chat legacy: [{body.agent}] {body.message[:60]}...")

        context_dict = safe_get_context_dict(body.context)
        result = await query_agent(
            agent_role=body.agent,
            message=body.message,
            context=context_dict
        )

        duration = time.time() - start_time
        logger.info(f"‚úÖ Agent [{body.agent}] responded in {duration:.2f}s")

        return {
            "success": True,
            "response": result,
            "agent": body.agent,
            "timestamp": datetime.now().isoformat(),
            "response_time": f"{duration:.2f}s"
        }

    except Exception as e:
        logger.error(f"‚ùå Agent chat failed: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Erreur agent {body.agent}: {str(e)}")

# ---- ENDPOINTS SP√âCIALIS√âS ----

@app.post("/agent/execute")
async def execute_code(request: CodeExecutionRequest):
    """Ex√©cution de code avec contexte"""
    try:
        prompt = f"""
EX√âCUTION DE CODE:
Langage: {request.language}
Code √† ex√©cuter:
```{request.language}
{request.code}
```

Instructions: Analyse ce code, explique ce qu'il fait, et retourne le r√©sultat attendu. Si il y a des erreurs, explique-les et propose des corrections.
"""
        
        if request.context:
            prompt = build_contextual_prompt(prompt, request.context)

        context_dict = safe_get_context_dict(request.context)
        result = await query_agent(
            agent_role=request.agent,
            message=prompt,
            context=context_dict
        )

        return {
            "success": True,
            "result": result,
            "language": request.language,
            "agent": request.agent,
            "metadata": {
                "execution_type": "code_execution",
                "code_length": len(request.code),
                "model_used": get_model_for_agent(request.agent)
            },
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"‚ùå Code execution error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur ex√©cution: {str(e)}")

@app.post("/agent/analyze")
async def analyze_code(request: CodeAnalysisRequest):
    """Analyse de code (review, debug, optimize, explain)"""
    try:
        analysis_prompts = {
            "review": f"REVIEW DE CODE:\nAnalyse ce code {request.language} et donne tes recommandations pour l'am√©liorer:",
            "optimize": f"OPTIMISATION:\nComment optimiser ce code {request.language} pour de meilleures performances:",
            "debug": f"DEBUG:\nAnalyse ce code {request.language} et identifie les probl√®mes potentiels:",
            "explain": f"EXPLICATION:\nExplique ce code {request.language} de mani√®re d√©taill√©e:"
        }
        
        prompt = analysis_prompts.get(request.analysis_type, analysis_prompts["review"])
        prompt += f"\n\n```{request.language}\n{request.code}\n```"

        result = await query_agent(
            agent_role="code-assistant",
            message=prompt,
            context={}
        )

        return {
            "success": True,
            "analysis": result,
            "analysis_type": request.analysis_type,
            "language": request.language,
            "model_used": get_model_for_agent("code-assistant"),
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"‚ùå Code analysis error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur analyse: {str(e)}")

@app.post("/agent/generate")
async def generate_code(request: CodeGenerationRequest):
    """G√©n√©ration de code avec contexte"""
    try:
        prompt = f"""
G√âN√âRATION DE CODE:
Langage: {request.language}
Description: {request.description}

Instructions: G√©n√®re du code {request.language} propre, bien comment√© et fonctionnel selon cette description. Inclus des exemples d'utilisation si pertinent.
"""
        
        if request.context:
            prompt = build_contextual_prompt(prompt, request.context)

        context_dict = safe_get_context_dict(request.context)
        result = await query_agent(
            agent_role="code-assistant",
            message=prompt,
            context=context_dict
        )

        return {
            "success": True,
            "generated_code": result,
            "language": request.language,
            "description": request.description,
            "model_used": get_model_for_agent("code-assistant"),
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"‚ùå Code generation error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur g√©n√©ration: {str(e)}")

# ---- WORKFLOWS ----

@app.get("/workflows/available")
async def get_available_workflows():
    """Liste des types de workflows disponibles"""
    try:
        workflows = workflow_orchestrator.get_available_workflows()
        return {
            "success": True,
            "workflows": workflows,
            "count": len(workflows),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Erreur workflows disponibles: {e}")
        return {"success": False, "workflows": [], "error": str(e)}

@app.post("/workflows/start")
async def start_workflow(request: WorkflowRequest):
    """D√©marre un nouveau workflow"""
    try:
        workflow_id = await workflow_orchestrator.start_workflow(
            request.workflow_type, 
            request.description
        )
        logger.info(f"üîÑ Workflow d√©marr√©: {workflow_id}")
        return {
            "success": True,
            "workflow_id": workflow_id,
            "type": request.workflow_type,
            "status": "started",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur d√©marrage workflow: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur workflow: {str(e)}")

@app.get("/workflows/{workflow_id}/status")
async def get_workflow_status(workflow_id: str):
    """Statut d'un workflow"""
    try:
        status = await workflow_orchestrator.get_workflow_status(workflow_id)
        return {
            "success": True,
            "workflow_id": workflow_id,
            "workflow_data": status,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur statut workflow: {e}")
        raise HTTPException(status_code=404, detail=f"Workflow {workflow_id} non trouv√©")

@app.post("/workflows/{workflow_id}/stop")
async def stop_workflow(workflow_id: str):
    """Arr√™te un workflow"""
    try:
        success = await workflow_orchestrator.stop_workflow(workflow_id)
        if success:
            return {
                "success": True,
                "workflow_id": workflow_id,
                "status": "stopped",
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=404, detail=f"Workflow {workflow_id} non trouv√©")
    except Exception as e:
        logger.error(f"‚ùå Erreur arr√™t workflow: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur arr√™t workflow: {str(e)}")

@app.get("/workflows")
async def list_running_workflows():
    """Liste des workflows en cours"""
    try:
        workflows = workflow_orchestrator.running_workflows
        return {
            "success": True,
            "running_workflows": workflows,
            "count": len(workflows),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur liste workflows: {e}")
        return {"success": False, "workflows": [], "error": str(e)}

# ---- UPLOAD FILES ----

@app.post("/kb/upload")
async def upload_kb_files(files: List[UploadFile] = File(...)):
    """Upload de fichiers de connaissance"""
    uploaded = []
    try:
        for file in files:
            # S√©curiser le nom de fichier
            safe_filename = "".join(c for c in file.filename if c.isalnum() or c in "._-")
            timestamp = int(time.time())
            safe_filename = f"{timestamp}_{safe_filename}"
            dest = os.path.join(UPLOAD_DIR, safe_filename)
            
            # Sauvegarder le fichier (version synchrone)
            with open(dest, 'wb') as f:
                content = await file.read()
                f.write(content)
            
            uploaded.append({
                "original_name": file.filename,
                "saved_name": safe_filename,
                "size": len(content),
                "content_type": file.content_type
            })
            
        logger.info(f"üìö Upload fichiers KB: {[f['saved_name'] for f in uploaded]}")
        return {
            "success": True,
            "uploaded_files": uploaded,
            "count": len(uploaded),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå Upload error: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur upload: {str(e)}")

# ---- SYSTEM ENDPOINTS ----

@app.get("/system/status")
async def system_status():
    """Statut d√©taill√© du syst√®me"""
    try:
        ollama_connected = await ollama_service.check_connection()
        models = await ollama_service.get_models()
        
        return {
            "system": {
                "status": "operational",
                "version": "1.4.0",
                "uptime": time.time(),
                "timestamp": datetime.now().isoformat()
            },
            "services": {
                "ollama": {
                    "connected": ollama_connected,
                    "url": OLLAMA_URL,
                    "models": models,
                    "default_model": DEFAULT_MODEL
                },
                "agents": {
                    "available": get_available_agents(),
                    "count": len(get_available_agents())
                },
                "workflows": {
                    "running": len(workflow_orchestrator.running_workflows),
                    "available_types": workflow_orchestrator.get_available_workflows()
                },
                "storage": {
                    "upload_dir": UPLOAD_DIR,
                    "files": len(os.listdir(UPLOAD_DIR)) if os.path.exists(UPLOAD_DIR) else 0
                }
            }
        }
    except Exception as e:
        logger.error(f"System status error: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur statut syst√®me: {str(e)}")

# ---- ERROR HANDLERS ----

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    logger.error(f"HTTP Exception: {exc.status_code} - {exc.detail}")
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail, "timestamp": datetime.now().isoformat()}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error(f"General Exception: {str(exc)}")
    logger.error(traceback.format_exc())
    return JSONResponse(
        status_code=500,
        content={"detail": f"Erreur interne: {str(exc)}", "timestamp": datetime.now().isoformat()}
    )

# ---- STARTUP/SHUTDOWN EVENTS ----

@app.on_event("startup")
async def startup_event():
    logger.info("üöÄ Backend Atelier IA d√©marr√© avec contexte conversationnel + Ollama")
    
    # Test de connexion Ollama
    ollama_connected = await ollama_service.check_connection()
    if ollama_connected:
        logger.info(f"‚úÖ Ollama connect√©: {len(ollama_service.available_models)} mod√®les disponibles")
        logger.info(f"ü§ñ Mod√®le par d√©faut: {DEFAULT_MODEL}")
    else:
        logger.warning("‚ö†Ô∏è Ollama non connect√© - mode fallback activ√©")
    
    logger.info(f"üìä {len(get_available_agents())} agents disponibles")
    logger.info(f"üîß {len(workflow_orchestrator.get_available_workflows())} types de workflows")
    logger.info("üß† M√©moire conversationnelle activ√©e")
    logger.info("‚úÖ Pr√™t √† recevoir des requ√™tes")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("üõë Arr√™t du backend Atelier IA")

# ---- DEV LAUNCHER ----

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)