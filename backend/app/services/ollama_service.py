# backend/app/services/ollama_service.py - VERSION AVANC√âE
import httpx
import json
import logging
import asyncio
from typing import Dict, Any, Optional, List
from ..utils.config import AGENT_ROLES, OLLAMA_CONFIG

logger = logging.getLogger(__name__)

class AdvancedOllamaService:
    def __init__(self):
        self.base_url = OLLAMA_CONFIG["base_url"]
        self.timeout = OLLAMA_CONFIG["timeout"]
        self.temperature = OLLAMA_CONFIG["temperature"]
        
        # Mapping des agents vers leurs mod√®les sp√©cialis√©s
        self.agent_models = {
            "visionnaire": "qwen2.5:3b",      # Excellent pour la vision produit
            "architecte": "deepseek-coder:6.7b",  # Sp√©cialis√© architecture
            "frontend_engineer": "deepseek-coder:6.7b",  # Code frontend
            "backend_engineer": "deepseek-r1:8b",    # Logique backend
            "database_specialist": "deepseek-r1:8b", # BDD et optimisation
            "designer_ui_ux": "qwen2.5:3b",     # Design et UX
            "critique": "deepseek-coder:6.7b",   # Analyse de code
            "optimiseur": "deepseek-coder:6.7b", # Performance
            "assistant": "qwen2.5:3b"           # G√©n√©raliste
        }
        
        # Prompts syst√®me optimis√©s par agent
        self.agent_prompts = {
            "visionnaire": """Tu es un expert en vision produit et strat√©gie. Ton r√¥le :
- Analyser les besoins utilisateurs en profondeur
- Proposer une vision produit claire et innovante
- Identifier les opportunit√©s de march√©
- D√©finir la roadmap de d√©veloppement
- Anticiper les tendances technologiques

R√©ponds de mani√®re structur√©e avec une vision business et technique √©quilibr√©e.""",

            "architecte": """Tu es un architecte logiciel senior avec 15+ ans d'exp√©rience. Ton expertise :
- Conception d'architectures scalables et maintenables
- Choix technologiques justifi√©s et patterns appropri√©s
- Microservices, APIs RESTful, bases de donn√©es
- Performance, s√©curit√© et monitoring
- DevOps et d√©ploiement

Fournis des solutions techniques pr√©cises avec diagrammes et code d'exemple.""",

            "frontend_engineer": """Tu es un d√©veloppeur frontend expert React/TypeScript/Next.js. Tes comp√©tences :
- Composants React modernes avec hooks
- TypeScript strict et interfaces typ√©es  
- State management (Zustand, Redux Toolkit)
- Styling (Tailwind, CSS-in-JS, animations)
- Performance (bundle optimization, lazy loading)
- Tests (Jest, Testing Library, Playwright)

G√©n√®re du code production-ready avec bonnes pratiques 2024.""",

            "backend_engineer": """Tu es un d√©veloppeur backend expert Python/FastAPI/Node.js. Tes domaines :
- APIs RESTful et GraphQL robustes
- Architecture microservices et monolithes modulaires
- Bases de donn√©es relationnelles et NoSQL
- Authentication, authorization, s√©curit√©
- Cache, queues, background jobs
- Tests d'int√©gration et monitoring

Produis du code backend scalable et s√©curis√©.""",

            "database_specialist": """Tu es un expert en bases de donn√©es et optimisation. Tes sp√©cialisations :
- Design de sch√©mas optimaux (PostgreSQL, MongoDB)
- Requ√™tes complexes et optimisation de performance
- Indexation strat√©gique et partitioning
- Migrations et versioning de sch√©ma
- R√©plication, backup et disaster recovery
- Monitoring et m√©triques de performance

Con√ßois des solutions de donn√©es robustes et performantes.""",

            "designer_ui_ux": """Tu es un designer UI/UX senior avec vision produit. Ton expertise :
- Design systems et composants r√©utilisables
- User experience et user journey mapping
- Interfaces modernes et accessibles
- Design responsive et mobile-first
- Prototypage et wireframing
- Tests utilisateurs et m√©triques UX

Cr√©e des exp√©riences utilisateur exceptionnelles et inclusives.""",

            "critique": """Tu es un expert en qualit√© logicielle et code review. Tes crit√®res d'analyse :
- Architecture et patterns de conception
- Qualit√© du code et respect des standards
- Performance et optimisation
- S√©curit√© et vuln√©rabilit√©s
- Testabilit√© et couverture de tests
- Maintenabilit√© et documentation

Fournis des analyses constructives avec suggestions d'am√©lioration concr√®tes.""",

            "optimiseur": """Tu es un expert en optimisation et performance. Tes domaines :
- Optimisation frontend (bundle, rendering, UX)
- Performance backend (requ√™tes, cache, algorithmes)
- Optimisation base de donn√©es (index, requ√™tes)
- Monitoring et m√©triques de performance
- Scalabilit√© horizontale et verticale
- Green coding et efficacit√© √©nerg√©tique

Propose des optimisations mesurables avec impact business."""
        }

    async def is_available(self) -> bool:
        """V√©rifie si Ollama est accessible"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags", timeout=5.0)
                return response.status_code == 200
        except Exception as e:
            logger.warning(f"Ollama non disponible: {e}")
            return False

    async def get_installed_models(self) -> List[str]:
        """R√©cup√®re la liste des mod√®les install√©s"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags", timeout=10.0)
                if response.status_code == 200:
                    models_data = response.json()
                    return [model['name'] for model in models_data.get('models', [])]
        except Exception as e:
            logger.error(f"Erreur r√©cup√©ration mod√®les: {e}")
        return []

    async def is_model_available(self, model_name: str) -> bool:
        """V√©rifie si un mod√®le sp√©cifique est disponible"""
        installed_models = await self.get_installed_models()
        return model_name in installed_models

    async def query_agent(self, agent_role: str, message: str, context: Dict[str, Any] = None) -> str:
        """Interface principale pour interroger un agent sp√©cialis√©"""
        context = context or {}
        
        # V√©rifier la disponibilit√© g√©n√©rale d'Ollama
        if not await self.is_available():
            return await self._fallback_response(agent_role, message)
        
        # R√©cup√©rer le mod√®le sp√©cialis√© pour cet agent
        model_name = self.agent_models.get(agent_role, "qwen2.5:3b")
        
        # V√©rifier que le mod√®le sp√©cialis√© est disponible
        if not await self.is_model_available(model_name):
            logger.warning(f"Mod√®le {model_name} non disponible pour {agent_role}")
            return await self._model_missing_response(agent_role, model_name, message)
        
        # Construire le prompt avec le contexte de l'agent
        system_prompt = self.agent_prompts.get(agent_role, "Tu es un assistant IA sp√©cialis√©.")
        full_prompt = f"{system_prompt}\n\nDemande utilisateur: {message}"
        
        # Ajouter le contexte si disponible
        if context.get("last_output"):
            full_prompt += f"\n\nContexte pr√©c√©dent: {context['last_output'][:500]}..."
        
        if context.get("project_type"):
            full_prompt += f"\n\nType de projet: {context['project_type']}"

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                # Param√®tres optimis√©s par type d'agent
                temperature = self._get_agent_temperature(agent_role)
                max_tokens = self._get_agent_max_tokens(agent_role)
                
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": model_name,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {
                            "temperature": temperature,
                            "top_p": 0.9,
                            "max_tokens": max_tokens,
                            "stop": ["<|im_end|>", "<|endoftext|>"]
                        }
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    generated_text = result.get("response", "Erreur: r√©ponse vide")
                    
                    # Post-traitement selon l'agent
                    return self._post_process_response(agent_role, generated_text)
                else:
                    logger.error(f"Erreur Ollama {model_name}: {response.status_code}")
                    return await self._fallback_response(agent_role, message)
                    
        except Exception as e:
            logger.error(f"Erreur lors de l'appel Ollama ({model_name}): {e}")
            return await self._fallback_response(agent_role, message)

    def _get_agent_temperature(self, agent_role: str) -> float:
        """Temp√©rature optimis√©e par type d'agent"""
        creative_agents = ["visionnaire", "designer_ui_ux"]
        technical_agents = ["architecte", "backend_engineer", "database_specialist"]
        
        if agent_role in creative_agents:
            return 0.8  # Plus cr√©atif
        elif agent_role in technical_agents:
            return 0.3  # Plus pr√©cis
        else:
            return 0.7  # √âquilibr√©

    def _get_agent_max_tokens(self, agent_role: str) -> int:
        """Nombre de tokens optimis√© par agent"""
        verbose_agents = ["architecte", "critique", "optimiseur"]
        if agent_role in verbose_agents:
            return 2000  # R√©ponses d√©taill√©es
        else:
            return 1200  # R√©ponses concises

    def _post_process_response(self, agent_role: str, response: str) -> str:
        """Post-traitement sp√©cialis√© par agent"""
        # Ajouter l'emoji et le nom de l'agent
        agent_info = AGENT_ROLES.get(agent_role, {})
        agent_name = agent_info.get("name", agent_role.title())
        
        # Emojis par agent
        emojis = {
            "visionnaire": "üîÆ",
            "architecte": "üèóÔ∏è", 
            "frontend_engineer": "‚öõÔ∏è",
            "backend_engineer": "üîß",
            "database_specialist": "üóÉÔ∏è",
            "designer_ui_ux": "üé®",
            "critique": "üîç",
            "optimiseur": "‚ö°",
            "assistant": "ü§ñ"
        }
        
        emoji = emojis.get(agent_role, "ü§ñ")
        header = f"{emoji} **[{agent_name}]**\n\n"
        
        return header + response.strip()

    async def _fallback_response(self, agent_role: str, message: str) -> str:
        """R√©ponse de secours si Ollama n'est pas disponible"""
        return f"""‚ö†Ô∏è **[Mode Simulation - {agent_role.title()}]**

Ollama non disponible. R√©ponse simul√©e pour: "{message}"

**Pour activer l'IA r√©elle sp√©cialis√©e:**
1. Installez Ollama: https://ollama.ai
2. Lancez: `ollama serve`
3. Installez les mod√®les: `python setup_ollama.py install`

**Mod√®le requis pour cet agent:** `{self.agent_models.get(agent_role, 'qwen2.5:3b')}`"""

    async def _model_missing_response(self, agent_role: str, model_name: str, message: str) -> str:
        """R√©ponse quand le mod√®le sp√©cialis√© est manquant"""
        return f"""‚ö†Ô∏è **[{agent_role.title()} - Mod√®le Manquant]**

Le mod√®le sp√©cialis√© `{model_name}` n'est pas install√©.

**Pour installer ce mod√®le:**
```bash
ollama pull {model_name}
```

**Ou installer tous les mod√®les automatiquement:**
```bash
python setup_ollama.py install
```

Demande re√ßue: "{message}"
*Cet agent sera pleinement fonctionnel une fois le mod√®le install√©.*"""

    async def get_agent_status(self) -> Dict[str, Any]:
        """Statut d√©taill√© de tous les agents"""
        if not await self.is_available():
            return {"ollama_available": False, "agents": {}}
        
        installed_models = await self.get_installed_models()
        agent_status = {}
        
        for agent_role, model_name in self.agent_models.items():
            agent_status[agent_role] = {
                "model": model_name,
                "available": model_name in installed_models,
                "priority": AGENT_ROLES.get(agent_role, {}).get("priority", False)
            }
        
        return {
            "ollama_available": True,
            "installed_models": installed_models,
            "agents": agent_status,
            "total_agents": len(agent_status),
            "available_agents": sum(1 for status in agent_status.values() if status["available"])
        }

# Singleton global
ollama_service = AdvancedOllamaService()