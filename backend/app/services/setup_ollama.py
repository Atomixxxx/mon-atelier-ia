import logging
import httpx
import subprocess
import time
import json
import asyncio
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from ..utils.config import AGENT_ROLES, get_required_models, get_priority_models

logger = logging.getLogger("setup_ollama")

class IntegratedOllamaSetup:
    def __init__(self):
        self.ollama_host = "http://localhost:11434"
        self.required_models = get_required_models()
        self.priority_models = get_priority_models()
        
        # Tailles des modÃ¨les (estimation en GB)
        self.model_sizes = {
            "qwen2.5:3b": 2.0,
            "deepseek-coder:6.7b": 4.1,
            "deepseek-r1:8b": 4.8,
            "mistral:7b": 4.1,
            "llama3.2:3b": 2.0
        }

    async def check_ollama_installed(self) -> bool:
        """VÃ©rifie si Ollama est installÃ©"""
        try:
            result = subprocess.run(['ollama', '--version'], 
                                  capture_output=True, 
                                  text=True, 
                                  timeout=10)
            if result.returncode == 0:
                version = result.stdout.strip()
                logger.info(f"âœ… Ollama installÃ©: {version}")
                return True
            else:
                logger.error("âŒ Ollama non accessible")
                return False
        except (subprocess.TimeoutExpired, FileNotFoundError):
            logger.error("âŒ Ollama non installÃ©")
            return False

    async def check_ollama_running(self) -> bool:
        """VÃ©rifie si le service Ollama fonctionne"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(self.ollama_host, timeout=5)
                if response.status_code == 200:
                    logger.info("âœ… Service Ollama actif")
                    return True
                else:
                    logger.warning(f"âš ï¸ Service Ollama rÃ©pond {response.status_code}")
                    return False
        except Exception as e:
            logger.warning(f"âš ï¸ Service Ollama non accessible: {e}")
            return False

    async def start_ollama_service(self) -> bool:
        """DÃ©marre le service Ollama"""
        logger.info("ğŸš€ DÃ©marrage du service Ollama...")
        try:
            # DÃ©marrer ollama serve en arriÃ¨re-plan
            subprocess.Popen(['ollama', 'serve'], 
                           stdout=subprocess.DEVNULL, 
                           stderr=subprocess.DEVNULL)
            
            # Attendre le dÃ©marrage
            for i in range(10):
                await asyncio.sleep(1)
                if await self.check_ollama_running():
                    logger.info("âœ… Service Ollama dÃ©marrÃ© avec succÃ¨s")
                    return True
                logger.info(f"â³ Attente dÃ©marrage... {i+1}/10")
            
            logger.error("âŒ Ã‰chec dÃ©marrage service Ollama")
            return False
            
        except Exception as e:
            logger.error(f"âŒ Erreur dÃ©marrage Ollama: {e}")
            return False

    async def get_installed_models(self) -> List[str]:
        """RÃ©cupÃ¨re les modÃ¨les installÃ©s"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.ollama_host}/api/tags", timeout=10)
                if response.status_code == 200:
                    models_data = response.json()
                    models = [model['name'] for model in models_data.get('models', [])]
                    logger.info(f"ğŸ“‹ {len(models)} modÃ¨les installÃ©s")
                    return models
                else:
                    logger.error(f"âŒ Erreur rÃ©cupÃ©ration modÃ¨les: {response.status_code}")
                    return []
        except Exception as e:
            logger.error(f"âŒ Erreur API modÃ¨les: {e}")
            return []

    async def pull_model_with_progress(self, model_name: str) -> bool:
        """TÃ©lÃ©charge un modÃ¨le avec suivi de progression"""
        logger.info(f"ğŸ“¥ TÃ©lÃ©chargement de {model_name}...")
        
        try:
            async with httpx.AsyncClient(timeout=600) as client:  # 10 minutes timeout
                async with client.stream(
                    "POST",
                    f"{self.ollama_host}/api/pull",
                    json={"name": model_name}
                ) as response:
                    
                    if response.status_code != 200:
                        logger.error(f"âŒ Erreur tÃ©lÃ©chargement {model_name}: {response.status_code}")
                        return False
                    
                    async for line in response.aiter_lines():
                        if line.strip():
                            try:
                                data = json.loads(line)
                                status = data.get("status", "")
                                
                                if "downloading" in status.lower():
                                    completed = data.get("completed", 0)
                                    total = data.get("total", 1)
                                    percent = (completed / total) * 100 if total > 0 else 0
                                    logger.info(f"â³ {model_name}: {percent:.1f}% ({completed}/{total} bytes)")
                                
                                elif "success" in status.lower():
                                    logger.info(f"âœ… {model_name} tÃ©lÃ©chargÃ© avec succÃ¨s")
                                    return True
                                    
                            except json.JSONDecodeError:
                                continue
                    
                    logger.info(f"âœ… {model_name} installÃ©")
                    return True
                    
        except Exception as e:
            logger.error(f"âŒ Erreur tÃ©lÃ©chargement {model_name}: {e}")
            return False

    async def install_priority_models(self) -> Tuple[bool, List[str]]:
        """Installe les modÃ¨les prioritaires pour le MVP"""
        logger.info("ğŸ“¦ Installation des modÃ¨les prioritaires...")
        
        installed_models = await self.get_installed_models()
        missing_priority = [
            model for model in self.priority_models 
            if model not in installed_models
        ]
        
        if not missing_priority:
            logger.info("âœ… Tous les modÃ¨les prioritaires sont installÃ©s")
            return True, []
        
        logger.info(f"ğŸ“¥ ModÃ¨les prioritaires Ã  installer: {missing_priority}")
        
        # Calculer l'espace requis
        total_size = sum(self.model_sizes.get(model, 5.0) for model in missing_priority)
        logger.info(f"ğŸ’¾ Espace requis: ~{total_size:.1f} GB")
        
        failed_models = []
        for model in missing_priority:
            logger.info(f"ğŸ”„ Installation de {model}...")
            if await self.pull_model_with_progress(model):
                logger.info(f"âœ… {model} installÃ© avec succÃ¨s")
            else:
                logger.error(f"âŒ Ã‰chec installation {model}")
                failed_models.append(model)
        
        success = len(failed_models) == 0
        return success, failed_models

    async def install_all_models(self) -> Tuple[bool, List[str]]:
        """Installe tous les modÃ¨les requis"""
        logger.info("ğŸ“¦ Installation complÃ¨te des modÃ¨les...")
        
        installed_models = await self.get_installed_models()
        missing_models = [
            model for model in self.required_models 
            if model not in installed_models
        ]
        
        if not missing_models:
            logger.info("âœ… Tous les modÃ¨les sont installÃ©s")
            return True, []
        
        # Calculer l'espace total requis
        total_size = sum(self.model_sizes.get(model, 5.0) for model in missing_models)
        logger.info(f"ğŸ’¾ Espace total requis: ~{total_size:.1f} GB")
        
        failed_models = []
        for model in missing_models:
            if await self.pull_model_with_progress(model):
                logger.info(f"âœ… {model} installÃ©")
            else:
                failed_models.append(model)
        
        success = len(failed_models) == 0
        return success, failed_models

    async def test_agent_models(self) -> Dict[str, bool]:
        """Teste tous les modÃ¨les d'agents"""
        logger.info("ğŸ§ª Test des modÃ¨les d'agents...")
        
        test_results = {}
        test_prompt = "RÃ©ponds juste 'OK' pour confirmer que tu fonctionnes."
        
        for agent_role, agent_info in AGENT_ROLES.items():
            model_name = agent_info["model"]
            logger.info(f"ğŸ” Test {agent_role} ({model_name})...")
            
            try:
                async with httpx.AsyncClient(timeout=30) as client:
                    response = await client.post(
                        f"{self.ollama_host}/api/generate",
                        json={
                            "model": model_name,
                            "prompt": test_prompt,
                            "stream": False
                        }
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        if result.get("response"):
                            test_results[agent_role] = True
                            logger.info(f"âœ… {agent_role} fonctionne")
                        else:
                            test_results[agent_role] = False
                            logger.warning(f"âš ï¸ {agent_role} rÃ©ponse vide")
                    else:
                        test_results[agent_role] = False
                        logger.error(f"âŒ {agent_role} erreur {response.status_code}")
                        
            except Exception as e:
                test_results[agent_role] = False
                logger.error(f"âŒ {agent_role} exception: {e}")
        
        working_agents = sum(test_results.values())
        total_agents = len(test_results)
        logger.info(f"ğŸ“Š Agents fonctionnels: {working_agents}/{total_agents}")
        
        return test_results

    async def get_detailed_status(self) -> Dict:
        """Statut dÃ©taillÃ© de la configuration"""
        status = {
            "ollama_installed": await self.check_ollama_installed(),
            "ollama_running": False,
            "installed_models": [],
            "required_models": self.required_models,
            "priority_models": self.priority_models,
            "missing_models": [],
            "missing_priority": [],
            "agents_status": {},
            "disk_usage_gb": 0,
            "setup_complete": False
        }
        
        if status["ollama_installed"]:
            status["ollama_running"] = await self.check_ollama_running()
            
            if status["ollama_running"]:
                status["installed_models"] = await self.get_installed_models()
                
                # Calculer les modÃ¨les manquants
                status["missing_models"] = [
                    model for model in self.required_models 
                    if model not in status["installed_models"]
                ]
                
                status["missing_priority"] = [
                    model for model in self.priority_models 
                    if model not in status["installed_models"]
                ]
                
                # Statut des agents
                for agent_role, agent_info in AGENT_ROLES.items():
                    model_name = agent_info["model"]
                    status["agents_status"][agent_role] = {
                        "model": model_name,
                        "available": model_name in status["installed_models"],
                        "priority": agent_info.get("priority", False)
                    }
                
                # Estimation usage disque
                status["disk_usage_gb"] = sum(
                    self.model_sizes.get(model, 5.0) 
                    for model in status["installed_models"]
                )
                
                # Setup complet si tous les modÃ¨les prioritaires sont prÃ©sents
                status["setup_complete"] = len(status["missing_priority"]) == 0
        
        return status

    async def setup_mvp_complete(self) -> bool:
        """Configuration complÃ¨te pour MVP"""
        logger.info("ğŸš€ Configuration MVP Ollama - DÃ©marrage")
        logger.info("=" * 50)
        
        # 1. VÃ©rifier installation Ollama
        if not await self.check_ollama_installed():
            logger.error("âŒ Ollama non installÃ©")
            logger.info("ğŸ“– Instructions d'installation:")
            logger.info("   1. Visitez: https://ollama.ai")
            logger.info("   2. Suivez les instructions pour votre OS")
            logger.info("   3. Relancez ce script")
            return False
        
        # 2. DÃ©marrer le service si nÃ©cessaire
        if not await self.check_ollama_running():
            if not await self.start_ollama_service():
                logger.error("âŒ Impossible de dÃ©marrer le service Ollama")
                return False
        
        # 3. Installer les modÃ¨les prioritaires
        success, failed = await self.install_priority_models()
        if not success:
            logger.error(f"âŒ Ã‰chec installation modÃ¨les: {failed}")
            return False
        
        # 4. Tester les agents prioritaires
        test_results = await self.test_agent_models()
        priority_agents = ["visionnaire", "architecte", "frontend_engineer"]
        priority_working = all(test_results.get(agent, False) for agent in priority_agents)
        
        if not priority_working:
            logger.error("âŒ Certains agents prioritaires ne fonctionnent pas")
            return False
        
        logger.info("ğŸ‰ Configuration MVP terminÃ©e avec succÃ¨s !")
        logger.info("âœ… Agents prioritaires fonctionnels")
        return True

    def print_status_report(self, status: Dict):
        """Affiche un rapport de statut dÃ©taillÃ©"""
        print("\n" + "=" * 60)
        print("ğŸ¤– RAPPORT STATUT OLLAMA - MON ATELIER IA")
        print("=" * 60)
        
        # Statut global
        print(f"ğŸ“¦ Ollama installÃ©: {'âœ…' if status['ollama_installed'] else 'âŒ'}")
        print(f"ğŸš€ Service actif: {'âœ…' if status['ollama_running'] else 'âŒ'}")
        print(f"ğŸ’¾ Usage disque: ~{status['disk_usage_gb']:.1f} GB")
        print(f"ğŸ¯ Setup MVP: {'âœ… Complet' if status['setup_complete'] else 'âš ï¸ Incomplet'}")
        
        # ModÃ¨les
        total_models = len(status['required_models'])
        installed_count = len(status['installed_models'])
        print(f"\nğŸ“Š ModÃ¨les: {installed_count}/{total_models} installÃ©s")
        
        if status['missing_priority']:
            print(f"âŒ ModÃ¨les prioritaires manquants: {', '.join(status['missing_priority'])}")
        else:
            print("âœ… Tous les modÃ¨les prioritaires installÃ©s")
        
        # Agents
        print(f"\nğŸ¤– AGENTS ({len(status['agents_status'])} total):")
        for agent_role, agent_status in status['agents_status'].items():
            available_icon = "âœ…" if agent_status['available'] else "âŒ"
            priority_icon = "â­" if agent_status['priority'] else "  "
            model_name = agent_status['model']
            print(f"   {available_icon} {priority_icon} {agent_role.ljust(20)} â†’ {model_name}")
        
        if status['missing_models']:
            print(f"\nğŸ“¥ Pour installer tous les modÃ¨les:")
            print(f"   python -m app.services.setup_ollama install_all")
        
        print("=" * 60)

# Fonctions utilitaires pour usage direct
async def setup_mvp_quick() -> bool:
    """Configuration rapide MVP"""
    setup = IntegratedOllamaSetup()
    return await setup.setup_mvp_complete()

async def check_status() -> Dict:
    """VÃ©rification rapide du statut"""
    setup = IntegratedOllamaSetup()
    return await setup.get_detailed_status()

async def install_priority_only() -> bool:
    """Installer seulement les modÃ¨les prioritaires"""
    setup = IntegratedOllamaSetup()
    
    if not await setup.check_ollama_running():
        if not await setup.start_ollama_service():
            return False
    
    success, failed = await setup.install_priority_models()
    return success

# Script principal
async def main():
    import sys
    
    setup = IntegratedOllamaSetup()
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "status":
            status = await setup.get_detailed_status()
            setup.print_status_report(status)
            
        elif command == "install_mvp":
            success = await setup.setup_mvp_complete()
            if success:
                print("ğŸ‰ Configuration MVP rÃ©ussie !")
            else:
                print("âŒ Ã‰chec configuration MVP")
                
        elif command == "install_all":
            success, failed = await setup.install_all_models()
            if success:
                print("ğŸ‰ Tous les modÃ¨les installÃ©s !")
            else:
                print(f"âŒ ModÃ¨les Ã©chouÃ©s: {failed}")
                
        elif command == "test":
            results = await setup.test_agent_models()
            working = sum(results.values())
            total = len(results)
            print(f"ğŸ§ª Tests: {working}/{total} agents fonctionnels")
            
        else:
            print("Usage: python setup_ollama.py [status|install_mvp|install_all|test]")
    else:
        # Configuration par dÃ©faut
        await setup.setup_mvp_complete()

if __name__ == "__main__":
    asyncio.run(main())