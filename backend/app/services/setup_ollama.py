import logging
import httpx
import subprocess
import time
import json
import asyncio
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from ..utils.config import AGENT_ROLES, get_required_models, get_priority_models

logger = logging.getLogger("setup_ollama")

class IntegratedOllamaSetup:
    def __init__(self):
        self.ollama_host = "http://localhost:11434"
        self.required_models = get_required_models()
        self.priority_models = get_priority_models()
        
        # Tailles des mod√®les (estimation en GB)
        self.model_sizes = {
            "qwen2.5:3b": 2.0,
            "deepseek-coder:6.7b": 4.1,
            "deepseek-r1:8b": 4.8,
            "mistral:7b": 4.1,
            "llama3.2:3b": 2.0
        }

    async def check_ollama_installed(self) -> bool:
        """V√©rifie si Ollama est install√©"""
        try:
            result = subprocess.run(['ollama', '--version'], 
                                  capture_output=True, 
                                  text=True, 
                                  timeout=10)
            if result.returncode == 0:
                version = result.stdout.strip()
                logger.info(f"‚úÖ Ollama install√©: {version}")
                return True
            else:
                logger.error("‚ùå Ollama non accessible")
                return False
        except (subprocess.TimeoutExpired, FileNotFoundError):
            logger.error("‚ùå Ollama non install√©")
            return False

    async def check_ollama_running(self) -> bool:
        """V√©rifie si le service Ollama fonctionne"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(self.ollama_host, timeout=5)
                if response.status_code == 200:
                    logger.info("‚úÖ Service Ollama actif")
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è Service Ollama r√©pond {response.status_code}")
                    return False
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Service Ollama non accessible: {e}")
            return False

    async def start_ollama_service(self) -> bool:
        """D√©marre le service Ollama"""
        logger.info("üöÄ D√©marrage du service Ollama...")
        try:
            # D√©marrer ollama serve en arri√®re-plan
            subprocess.Popen(['ollama', 'serve'], 
                           stdout=subprocess.DEVNULL, 
                           stderr=subprocess.DEVNULL)
            
            # Attendre le d√©marrage
            for i in range(10):
                await asyncio.sleep(1)
                if await self.check_ollama_running():
                    logger.info("‚úÖ Service Ollama d√©marr√© avec succ√®s")
                    return True
                logger.info(f"‚è≥ Attente d√©marrage... {i+1}/10")
            
            logger.error("‚ùå √âchec d√©marrage service Ollama")
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©marrage Ollama: {e}")
            return False

    async def get_installed_models(self) -> List[str]:
        """R√©cup√®re les mod√®les install√©s"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.ollama_host}/api/tags", timeout=10)
                if response.status_code == 200:
                    models_data = response.json()
                    models = [model['name'] for model in models_data.get('models', [])]
                    logger.info(f"üìã {len(models)} mod√®les install√©s")
                    return models
                else:
                    logger.error(f"‚ùå Erreur r√©cup√©ration mod√®les: {response.status_code}")
                    return []
        except Exception as e:
            logger.error(f"‚ùå Erreur API mod√®les: {e}")
            return []

    async def pull_model_with_progress(self, model_name: str) -> bool:
        """T√©l√©charge un mod√®le avec suivi de progression"""
        logger.info(f"üì• T√©l√©chargement de {model_name}...")
        
        try:
            async with httpx.AsyncClient(timeout=600) as client:  # 10 minutes timeout
                async with client.stream(
                    "POST",
                    f"{self.ollama_host}/api/pull",
                    json={"name": model_name}
                ) as response:
                    
                    if response.status_code != 200:
                        logger.error(f"‚ùå Erreur t√©l√©chargement {model_name}: {response.status_code}")
                        return False
                    
                    async for line in response.aiter_lines():
                        if line.strip():
                            try:
                                data = json.loads(line)
                                status = data.get("status", "")
                                
                                if "downloading" in status.lower():
                                    completed = data.get("completed", 0)
                                    total = data.get("total", 1)
                                    percent = (completed / total) * 100 if total > 0 else 0
                                    logger.info(f"‚è≥ {model_name}: {percent:.1f}% ({completed}/{total} bytes)")
                                
                                elif "success" in status.lower():
                                    logger.info(f"‚úÖ {model_name} t√©l√©charg√© avec succ√®s")
                                    return True
                                    
                            except json.JSONDecodeError:
                                continue
                    
                    logger.info(f"‚úÖ {model_name} install√©")
                    return True
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur t√©l√©chargement {model_name}: {e}")
            return False

    async def install_priority_models(self) -> Tuple[bool, List[str]]:
        """Installe les mod√®les prioritaires pour le MVP"""
        logger.info("üì¶ Installation des mod√®les prioritaires...")
        
        installed_models = await self.get_installed_models()
        missing_priority = [
            model for model in self.priority_models 
            if model not in installed_models
        ]
        
        if not missing_priority:
            logger.info("‚úÖ Tous les mod√®les prioritaires sont install√©s")
            return True, []
        
        logger.info(f"üì• Mod√®les prioritaires √† installer: {missing_priority}")
        
        # Calculer l'espace requis
        total_size = sum(self.model_sizes.get(model, 5.0) for model in missing_priority)
        logger.info(f"üíæ Espace requis: ~{total_size:.1f} GB")
        
        failed_models = []
        for model in missing_priority:
            logger.info(f"üîÑ Installation de {model}...")
            if await self.pull_model_with_progress(model):
                logger.info(f"‚úÖ {model} install√© avec succ√®s")
            else:
                logger.error(f"‚ùå √âchec installation {model}")
                failed_models.append(model)
        
        success = len(failed_models) == 0
        return success, failed_models

    async def install_all_models(self) -> Tuple[bool, List[str]]:
        """Installe tous les mod√®les requis"""
        logger.info("üì¶ Installation compl√®te des mod√®les...")
        
        installed_models = await self.get_installed_models()
        missing_models = [
            model for model in self.required_models 
            if model not in installed_models
        ]
        
        if not missing_models:
            logger.info("‚úÖ Tous les mod√®les sont install√©s")
            return True, []
        
        # Calculer l'espace total requis
        total_size = sum(self.model_sizes.get(model, 5.0) for model in missing_models)
        logger.info(f"üíæ Espace total requis: ~{total_size:.1f} GB")
        
        failed_models = []
        for model in missing_models:
            if await self.pull_model_with_progress(model):
                logger.info(f"‚úÖ {model} install√©")
            else:
                failed_models.append(model)
        
        success = len(failed_models) == 0
        return success, failed_models

    async def test_agent_models(self) -> Dict[str, bool]:
        """Teste tous les mod√®les d'agents"""
        logger.info("üß™ Test des mod√®les d'agents...")
        
        test_results = {}
        test_prompt = "R√©ponds juste 'OK' pour confirmer que tu fonctionnes."
        
        for agent_role, agent_info in AGENT_ROLES.items():
            model_name = agent_info["model"]
            logger.info(f"üîç Test {agent_role} ({model_name})...")
            
            try:
                async with httpx.AsyncClient(timeout=30) as client:
                    response = await client.post(
                        f"{self.ollama_host}/api/generate",
                        json={
                            "model": model_name,
                            "prompt": test_prompt,
                            "stream": False
                        }
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        if result.get("response"):
                            test_results[agent_role] = True
                            logger.info(f"‚úÖ {agent_role} fonctionne")
                        else:
                            test_results[agent_role] = False
                            logger.warning(f"‚ö†Ô∏è {agent_role} r√©ponse vide")
                    else:
                        test_results[agent_role] = False
                        logger.error(f"‚ùå {agent_role} erreur {response.status_code}")
                        
            except Exception as e:
                test_results[agent_role] = False
                logger.error(f"‚ùå {agent_role} exception: {e}")
        
        working_agents = sum(test_results.values())
        total_agents = len(test_results)
        logger.info(f"üìä Agents fonctionnels: {working_agents}/{total_agents}")
        
        return test_results

    async def get_detailed_status(self) -> Dict:
        """Statut d√©taill√© de la configuration"""
        status = {
            "ollama_installed": await self.check_ollama_installed(),
            "ollama_running": False,
            "installed_models": [],
            "required_models": self.required_models,
            "priority_models": self.priority_models,
            "missing_models": [],
            "missing_priority": [],
            "agents_status": {},
            "disk_usage_gb": 0,
            "setup_complete": False
        }
        
        if status["ollama_installed"]:
            status["ollama_running"] = await self.check_ollama_running()
            
            if status["ollama_running"]:
                status["installed_models"] = await self.get_installed_models()
                
                # Calculer les mod√®les manquants
                status["missing_models"] = [
                    model for model in self.required_models 
                    if model not in status["installed_models"]
                ]
                
                status["missing_priority"] = [
                    model for model in self.priority_models 
                    if model not in status["installed_models"]
                ]
                
                # Statut des agents
                for agent_role, agent_info in AGENT_ROLES.items():
                    model_name = agent_info["model"]
                    status["agents_status"][agent_role] = {
                        "model": model_name,
                        "available": model_name in status["installed_models"],
                        "priority": agent_info.get("priority", False)
                    }
                
                # Estimation usage disque
                status["disk_usage_gb"] = sum(
                    self.model_sizes.get(model, 5.0) 
                    for model in status["installed_models"]
                )
                
                # Setup complet si tous les mod√®les prioritaires sont pr√©sents
                status["setup_complete"] = len(status["missing_priority"]) == 0
        
        return status

    async def setup_mvp_complete(self) -> bool:
        """Configuration compl√®te pour MVP"""
        logger.info("üöÄ Configuration MVP Ollama - D√©marrage")
        logger.info("=" * 50)
        
        # 1. V√©rifier installation Ollama
        if not await self.check_ollama_installed():
            logger.error("‚ùå Ollama non install√©")
            logger.info("üìñ Instructions d'installation:")
            logger.info("   1. Visitez: https://ollama.ai")
            logger.info("   2. Suivez les instructions pour votre OS")
            logger.info("   3. Relancez ce script")
            return False
        
        # 2. D√©marrer le service si n√©cessaire
        if not await self.check_ollama_running():
            if not await self.start_ollama_service():
                logger.error("‚ùå Impossible de d√©marrer le service Ollama")
                return False
        
        # 3. Installer les mod√®les prioritaires
        success, failed = await self.install_priority_models()
        if not success:
            logger.error(f"‚ùå √âchec installation mod√®les: {failed}")
            return False
        
        # 4. Tester les agents prioritaires
        test_results = await self.test_agent_models()
        priority_agents = ["visionnaire", "architecte", "frontend_engineer"]
        priority_working = all(test_results.get(agent, False) for agent in priority_agents)
        
        if not priority_working:
            logger.error("‚ùå Certains agents prioritaires ne fonctionnent pas")
            return False
        
        logger.info("üéâ Configuration MVP termin√©e avec succ√®s !")
        logger.info("‚úÖ Agents prioritaires fonctionnels")
        return True

    def print_status_report(self, status: Dict):
        """Affiche un rapport de statut d√©taill√©"""
        print("\n" + "=" * 60)
        print("ü§ñ RAPPORT STATUT OLLAMA - MON ATELIER IA")
        print("=" * 60)
        
        # Statut global
        print(f"üì¶ Ollama install√©: {'‚úÖ' if status['ollama_installed'] else '‚ùå'}")
        print(f"üöÄ Service actif: {'‚úÖ' if status['ollama_running'] else '‚ùå'}")
        print(f"üíæ Usage disque: ~{status['disk_usage_gb']:.1f} GB")
        print(f"üéØ Setup MVP: {'‚úÖ Complet' if status['setup_complete'] else '‚ö†Ô∏è Incomplet'}")
        
        # Mod√®les
        total_models = len(status['required_models'])
        installed_count = len(status['installed_models'])
        print(f"\nüìä Mod√®les: {installed_count}/{total_models} install√©s")
        
        if status['missing_priority']:
            print(f"‚ùå Mod√®les prioritaires manquants: {', '.join(status['missing_priority'])}")
        else:
            print("‚úÖ Tous les mod√®les prioritaires install√©s")
        
        # Agents
        print(f"\nü§ñ AGENTS ({len(status['agents_status'])} total):")
        for agent_role, agent_status in status['agents_status'].items():
            available_icon = "‚úÖ" if agent_status['available'] else "‚ùå"
            priority_icon = "‚≠ê" if agent_status['priority'] else "  "
            model_name = agent_status['model']
            print(f"   {available_icon} {priority_icon} {agent_role.ljust(20)} ‚Üí {model_name}")
        
        if status['missing_models']:
            print(f"\nüì• Pour installer tous les mod√®les:")
            print(f"   python -m app.services.setup_ollama install_all")
        
        print("=" * 60)

# Fonctions utilitaires pour usage direct
async def setup_mvp_quick() -> bool:
    """Configuration rapide MVP"""
    setup = IntegratedOllamaSetup()
    return await setup.setup_mvp_complete()

async def check_status() -> Dict:
    """V√©rification rapide du statut"""
    setup = IntegratedOllamaSetup()
    return await setup.get_detailed_status()

async def install_priority_only() -> bool:
    """Installer seulement les mod√®les prioritaires"""
    setup = IntegratedOllamaSetup()
    
    if not await setup.check_ollama_running():
        if not await setup.start_ollama_service():
            return False
    
    success, failed = await setup.install_priority_models()
    return success

# Script principal
async def main():
    import sys
    
    setup = IntegratedOllamaSetup()
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "status":
            status = await setup.get_detailed_status()
            setup.print_status_report(status)
            
        elif command == "install_mvp":
            success = await setup.setup_mvp_complete()
            if success:
                print("üéâ Configuration MVP r√©ussie !")
            else:
                print("‚ùå √âchec configuration MVP")
                
        elif command == "install_all":
            success, failed = await setup.install_all_models()
            if success:
                print("üéâ Tous les mod√®les install√©s !")
            else:
                print(f"‚ùå Mod√®les √©chou√©s: {failed}")
                
        elif command == "test":
            results = await setup.test_agent_models()
            working = sum(results.values())
            total = len(results)
            print(f"üß™ Tests: {working}/{total} agents fonctionnels")
            
        else:
            print("Usage: python setup_ollama.py [status|install_mvp|install_all|test]")
    else:
        # Configuration par d√©faut
        await setup.setup_mvp_complete()

if __name__ == "__main__":
    asyncio.run(main())