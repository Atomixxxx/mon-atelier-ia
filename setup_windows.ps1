# setup_windows_fixed.ps1 - Configuration Mon Atelier IA pour Windows (VERSION CORRIG√âE)
param(
    [switch]$Force,
    [switch]$SkipOllama
)

Write-Host "üöÄ Configuration Avanc√©e Mon Atelier IA - Windows" -ForegroundColor Green
Write-Host "=================================================" -ForegroundColor Green

# Fonctions pour afficher des messages color√©s
function Write-Status {
    param($Message)
    Write-Host "‚ÑπÔ∏è  $Message" -ForegroundColor Cyan
}

function Write-Success {
    param($Message)
    Write-Host "‚úÖ $Message" -ForegroundColor Green
}

function Write-Warning {
    param($Message)
    Write-Host "‚ö†Ô∏è  $Message" -ForegroundColor Yellow
}

function Write-ErrorMsg {
    param($Message)
    Write-Host "‚ùå $Message" -ForegroundColor Red
}

# V√©rifier que nous sommes dans le bon r√©pertoire
if (-not (Test-Path "app")) {
    Write-ErrorMsg "Lancez ce script depuis le dossier backend/"
    exit 1
}

Write-Status "V√©rification de l'environnement..."

# V√©rifier Python
try {
    $pythonVersion = python --version 2>&1
    Write-Success "Python d√©tect√©: $pythonVersion"
} catch {
    Write-ErrorMsg "Python non trouv√©. Installez Python 3.8+ depuis python.org"
    exit 1
}

# V√©rifier/cr√©er l'environnement virtuel
if (-not (Test-Path "venv") -and -not (Test-Path ".venv")) {
    Write-Status "Cr√©ation de l'environnement virtuel..."
    python -m venv venv
    Write-Success "Environnement virtuel cr√©√©"
} else {
    Write-Warning "Environnement virtuel existant d√©tect√©"
}

# Activer l'environnement virtuel
Write-Status "Activation de l'environnement virtuel..."
if (Test-Path "venv/Scripts/Activate.ps1") {
    & "./venv/Scripts/Activate.ps1"
    Write-Success "Environnement virtuel activ√©"
} elseif (Test-Path ".venv/Scripts/Activate.ps1") {
    & "./.venv/Scripts/Activate.ps1"
    Write-Success "Environnement virtuel activ√©"
} else {
    Write-ErrorMsg "Impossible d'activer l'environnement virtuel"
    exit 1
}

# Mettre √† jour pip et installer les d√©pendances
Write-Status "Installation des d√©pendances..."
python -m pip install --upgrade pip --quiet
pip install fastapi uvicorn httpx pydantic python-multipart aiofiles python-dotenv pytest pytest-asyncio --quiet
Write-Success "D√©pendances install√©es"

# Cr√©er la structure de dossiers
Write-Status "Cr√©ation de la structure de dossiers..."
$folders = @(
    "data/projects",
    "data/conversations", 
    "data/workflows",
    "data/backups",
    "logs",
    "uploads",
    "tests"
)

foreach ($folder in $folders) {
    if (-not (Test-Path $folder)) {
        New-Item -Path $folder -ItemType Directory -Force | Out-Null
    }
}
Write-Success "Structure de dossiers cr√©√©e"

# Cr√©er/mettre √† jour le fichier .env
Write-Status "Configuration du fichier .env..."
$envContent = @"
# Configuration Mon Atelier IA - Windows
HOST=0.0.0.0
PORT=8000
DEBUG=True

# Ollama Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_TIMEOUT=60
DEFAULT_TEMPERATURE=0.7

# Storage
DATA_DIR=./data
BACKUP_DIR=./backups
MAX_CONTEXT_LENGTH=2000

# Agents
PRIORITY_AGENTS=visionnaire,architecte,frontend_engineer

# CORS
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173,http://localhost:8080

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/app.log
"@

if (-not (Test-Path ".env") -or $Force) {
    $envContent | Out-File -FilePath ".env" -Encoding UTF8
    Write-Success "Fichier .env cr√©√©"
} else {
    Write-Warning "Fichier .env existant (utilisez -Force pour √©craser)"
}

# Mise √† jour du service AI
Write-Status "Mise √† jour du service AI..."
$aiServiceContent = @'
# backend/app/services/ai_service.py - VERSION SIMPLE MISE √Ä JOUR
"""
Service d'agents IA - Version am√©lior√©e avec mod√®les sp√©cialis√©s
"""

import asyncio
import httpx
from typing import Dict, List, Any
from ..utils.config import AGENT_ROLES

class SimpleOllamaService:
    def __init__(self):
        self.base_url = "http://localhost:11434"
        
        # Mod√®les sp√©cialis√©s
        self.agent_models = {
            "visionnaire": "qwen2.5:3b",
            "architecte": "deepseek-coder:6.7b", 
            "frontend_engineer": "deepseek-coder:6.7b",
            "backend_engineer": "deepseek-r1:8b",
            "database_specialist": "deepseek-r1:8b",
            "designer_ui_ux": "qwen2.5:3b",
            "critique": "deepseek-coder:6.7b",
            "optimiseur": "deepseek-coder:6.7b",
            "assistant": "qwen2.5:3b"
        }
        
        # Prompts sp√©cialis√©s
        self.agent_prompts = {
            "visionnaire": "Tu es un expert en vision produit. Analyse la demande et propose une vision innovante avec technologies et roadmap.",
            "architecte": "Tu es un architecte logiciel senior. Con√ßois une architecture technique d√©taill√©e avec justifications.",
            "frontend_engineer": "Tu es un d√©veloppeur React/TypeScript expert. Cr√©e du code frontend moderne et fonctionnel.",
            "backend_engineer": "Tu es un d√©veloppeur backend Python/FastAPI. D√©veloppe des APIs robustes et scalables.",
            "database_specialist": "Tu es un expert en bases de donn√©es. Optimise les sch√©mas et requ√™tes pour la performance.",
            "designer_ui_ux": "Tu es un designer UI/UX. Cr√©e des interfaces modernes et une exp√©rience utilisateur exceptionnelle.",
            "critique": "Tu es un analyste code expert. Fournis une critique constructive avec suggestions d'am√©lioration.",
            "optimiseur": "Tu es un expert en optimisation. Am√©liore les performances et la qualit√© du code."
        }

    async def is_available(self) -> bool:
        """V√©rifie si Ollama est accessible"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags", timeout=5.0)
                return response.status_code == 200
        except:
            return False

    async def query_agent(self, agent_role: str, message: str, context: Dict[str, Any] = None) -> str:
        """Interface principale"""
        context = context or {}
        
        if not await self.is_available():
            return await self._fallback_response(agent_role, message)
        
        # R√©cup√©rer le mod√®le sp√©cialis√©
        model_name = self.agent_models.get(agent_role, "qwen2.5:3b")
        system_prompt = self.agent_prompts.get(agent_role, "Tu es un assistant IA sp√©cialis√©.")
        
        # Construire le prompt
        full_prompt = f"{system_prompt}\n\nDemande: {message}"
        if context.get("last_output"):
            full_prompt += f"\n\nContexte: {context['last_output'][:300]}..."

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": model_name,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "max_tokens": 1500
                        }
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    generated_text = result.get("response", "Erreur: r√©ponse vide")
                    
                    # Formatage simple
                    emoji = {"visionnaire": "üîÆ", "architecte": "üèóÔ∏è", "frontend_engineer": "‚öõÔ∏è"}.get(agent_role, "ü§ñ")
                    return f"{emoji} **[{agent_role.title()}]**\n\n{generated_text}"
                else:
                    return await self._fallback_response(agent_role, message)
                    
        except Exception as e:
            return await self._fallback_response(agent_role, message)

    async def _fallback_response(self, agent_role: str, message: str) -> str:
        """R√©ponse de simulation si Ollama indisponible"""
        model_name = self.agent_models.get(agent_role, "qwen2.5:3b")
        return f"""‚ö†Ô∏è **[Mode Simulation - {agent_role.title()}]**

Ollama non disponible. Pour activer l'IA r√©elle:

1. Installez Ollama: https://ollama.ai
2. Lancez: `ollama serve`  
3. Installez le mod√®le: `ollama pull {model_name}`

Message re√ßu: "{message}"

*R√©ponse simul√©e - sera remplac√©e par l'IA r√©elle une fois configur√©e.*"""

# Instance globale
simple_ollama_service = SimpleOllamaService()

# Fonctions compatibles avec le code existant
def get_available_agents() -> List[str]:
    """Retourne la liste des agents disponibles"""
    return list(AGENT_ROLES.keys())

async def query_agent(agent_role: str, message: str, context: Dict[str, Any] = None) -> str:
    """Interface principale - COMPATIBLE avec le code actuel"""
    return await simple_ollama_service.query_agent(agent_role, message, context)

def get_agent_capabilities(agent_role: str) -> Dict[str, Any]:
    """Retourne les capacit√©s d'un agent"""
    return AGENT_ROLES.get(agent_role, {})

def validate_agent_role(agent_role: str) -> bool:
    """Valide qu'un r√¥le d'agent existe"""
    return agent_role in AGENT_ROLES
'@

# Sauvegarder l'ancien fichier
if (Test-Path "app/services/ai_service.py") {
    Copy-Item "app/services/ai_service.py" "app/services/ai_service.py.backup" -Force
    Write-Warning "Ancien ai_service.py sauvegard√©"
}

$aiServiceContent | Out-File -FilePath "app/services/ai_service.py" -Encoding UTF8
Write-Success "Service AI mis √† jour"

# Cr√©er un script de test simple
Write-Status "Cr√©ation du script de test..."
$testScript = @'
# test_quick.py - Test rapide
import asyncio
import sys
import requests

async def quick_test():
    print("üß™ Test rapide Mon Atelier IA")
    print("=" * 40)
    
    try:
        # Test imports
        sys.path.append('.')
        from app.services.ai_service import query_agent, get_available_agents
        print("‚úÖ Modules import√©s avec succ√®s")
        
        # Test liste des agents
        agents = get_available_agents()
        print(f"‚úÖ {len(agents)} agents disponibles")
        
        # Test Ollama
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                print(f"‚úÖ Ollama actif avec {len(models)} mod√®les")
                
                # Test agent
                result = await query_agent("visionnaire", "Test simple")
                if "Mode Simulation" in result:
                    print("‚ö†Ô∏è Agent en mode simulation")
                else:
                    print("‚úÖ Agent fonctionne (IA r√©elle)")
                    
            else:
                print("‚ö†Ô∏è Ollama r√©pond incorrectement")
                
        except requests.exceptions.RequestException:
            print("‚ö†Ô∏è Ollama non accessible - Mode simulation OK")
            result = await query_agent("visionnaire", "Test simulation")
            print("‚úÖ Mode simulation fonctionne")
        
        print("\nüéâ Configuration fonctionnelle !")
        print("\nPour d√©marrer le serveur:")
        print("uvicorn app.main:app --reload")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(quick_test())
    exit(0 if success else 1)
'@

$testScript | Out-File -FilePath "test_quick.py" -Encoding UTF8
Write-Success "Script de test cr√©√©"

# Test de la configuration
Write-Status "Test de la configuration..."
try {
    python test_quick.py
    if ($LASTEXITCODE -eq 0) {
        Write-Success "Configuration valid√©e avec succ√®s"
    } else {
        Write-Warning "Probl√®mes d√©tect√©s dans la configuration"
    }
} catch {
    Write-Warning "Impossible de tester la configuration"
}

# Instructions finales
Write-Host ""
Write-Host "üéâ Configuration termin√©e !" -ForegroundColor Green
Write-Host "=========================" -ForegroundColor Green
Write-Host ""
Write-Host "Prochaines √©tapes:" -ForegroundColor Yellow
Write-Host "1. D√©marrer le serveur:" -ForegroundColor White
Write-Host "   uvicorn app.main:app --reload" -ForegroundColor Cyan
Write-Host ""
Write-Host "2. Tester:" -ForegroundColor White
Write-Host "   python test_quick.py" -ForegroundColor Cyan
Write-Host ""
Write-Host "3. Interface web:" -ForegroundColor White
Write-Host "   http://localhost:8000/docs" -ForegroundColor Cyan
Write-Host ""

if (-not $SkipOllama) {
    Write-Host "4. Pour l'IA r√©elle, installez Ollama et les mod√®les:" -ForegroundColor Yellow
    Write-Host "   - T√©l√©chargez Ollama: https://ollama.ai" -ForegroundColor Cyan
    Write-Host "   - Lancez: ollama serve" -ForegroundColor Cyan
    Write-Host "   - Installez: ollama pull qwen2.5:3b" -ForegroundColor Cyan
}

Write-Host ""
Write-Host "‚úÖ Pr√™t pour le d√©veloppement !" -ForegroundColor Green